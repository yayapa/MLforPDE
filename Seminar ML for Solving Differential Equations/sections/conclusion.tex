\section{Conclusion}
\label{sec:conclusion}
In this seminar work, we report about the diversity of the approaches that apply different machine learning techniques to solve differential equations. We point out three main categories: theory-guided to include physics knowledge into machine learning models, neural operators to improve their generalizability, and high-dimensional approaches to get rid of the curse of dimensionality. 

We observe that the plethora of techniques introduced in other spheres of computer science such as fully-connected NN, CNN, RNN, LSTMs, Transformers, etc., have found their application in mathematics to solve differential equations more accurately and effectively. Moreover, the state-of-the-art works attempt to mitigate black-box machine learning models usually based only on data by incorporating physical laws or knowledge of the underlying equation. Furthermore, the current models tend to be hybrid and use both numerical and machine learning methods for better performance and generalization. Another direction of research is searching for a universal neural differential operator that generalizes the whole differential operator in the equation and allows resolution and boundary condition independent solutions.

Based on exploration of state-of-the-art approaches, we assume that the further direction of future works can be focused on improving the generalizability by solving differential equations. This can be achieved by theory-guided hybrid models that can replace the computationally intensive parts of numerical solvers with faster neural networks. Another interesting direction of future works can be the discovering of the universal neural operators for large groups of equations. 
